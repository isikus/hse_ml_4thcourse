{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgIdI9eYCMFC"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--GB1uCfCihW"
   },
   "source": [
    "### Executed on Google Colab servers [here](https://colab.research.google.com/drive/1Xn-qh1mYlMifeIh9Oi_DdColGJEUnOMZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m_C0mC4NAcNS"
   },
   "source": [
    "## 0. Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-yME_J16CWfR",
    "outputId": "0db4ff0c-3af8-45f4-d50a-e85617d59513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-10-10 18:53:08--  http://www.gutenberg.org/files/2600/2600-0.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3359549 (3.2M) [text/plain]\n",
      "Saving to: ‘2600-0.txt’\n",
      "\n",
      "2600-0.txt          100%[===================>]   3.20M  1.46MB/s    in 2.2s    \n",
      "\n",
      "2019-10-10 18:53:10 (1.46 MB/s) - ‘2600-0.txt’ saved [3359549/3359549]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "!mv 2600-0.txt peace.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ea55e8RBCMFW",
    "outputId": "412dfe8b-70b4-4156-bae4-4fd78fc9220b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227579"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r').read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OS8jnoA4Af4K"
   },
   "source": [
    "## 1. Make text lowercase and remove all punctuation except spaces and dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Halo6rxL4z6K"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37tQhcjNCMFg"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower().replace(\"\\n\", \" \")\n",
    "    text = re.sub(r\"[^.' \\-\\w]\", \"\", text)\n",
    "    text = re.sub(r\"\\.(?!\\s)\", \"\", text)\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cZEHSk-444gc",
    "outputId": "b2660bff-4bf4-40f1-d059-5de47f4c5a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 31.7 ms, total: 267 ms\n",
      "Wall time: 269 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "text = preprocess_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lIym1464xZeP",
    "outputId": "560d9c5c-f696-42c8-d1bd-c62ba4bc6573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3128031"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yZc7VovzCMFn"
   },
   "outputs": [],
   "source": [
    "text = text.split('.')\n",
    "text = [x.strip() for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wNsQjTL70VWm",
    "outputId": "6d329aee-c03e-4932-c335-c6f13a1782be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26766"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Vfz6fDVArHn"
   },
   "source": [
    "## 2. Tokenize text by BPE with vocab_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8IEfywRnyiZN"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from sklearn.base import TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J3K_nk9BCMFu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super(BPE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.stok = OrderedDict()\n",
    "        self.ktos = OrderedDict()\n",
    "        \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        text: list of strings \n",
    "        \"\"\"\n",
    "        uniquechars = set(list(\"\".join(text)))\n",
    "        for c in uniquechars:\n",
    "          self.stok[c] = c\n",
    "        \n",
    "        while len(self.stok) < self.vocab_size:\n",
    "            new_count = [s[i]+s[i+1] for s in text for i in range(len(s)-1)]\n",
    "            new_count = Counter(new_count)\n",
    "            new_token = new_count.most_common(1)[0][0]\n",
    "            new_ch = chr(max([ord(c) for c in list(self.stok.values())]) + 1)\n",
    "            \n",
    "            self.stok[new_token] = new_ch\n",
    "            \n",
    "            text = [s.replace(new_token, new_ch) for s in text]\n",
    "        \n",
    "        for k in self.stok:\n",
    "          self.ktos[self.stok[k]] = k\n",
    "        return self\n",
    "    \n",
    "    def transform(self, text):\n",
    "        \"\"\"\n",
    "        convert text to a sequence of token ids\n",
    "        text: list of strings\n",
    "        \"\"\"\n",
    "        gluer = ord(\"\\n\")\n",
    "        used = set([ord(ch) for ch in self.stok.values()])\n",
    "        for s in text:\n",
    "          used.update(set([ord(ch) for ch in s]))\n",
    "        if gluer in used:\n",
    "          gluer = max(list(used)) + 1\n",
    "        gluer = chr(gluer)\n",
    "        text = gluer.join(text)\n",
    "        for subst in self.stok:\n",
    "          if subst != self.stok[subst]:\n",
    "            text = text.replace(subst, self.stok[subst])\n",
    "        text = text.split(gluer)\n",
    "        text = [[ord(ch) for ch in s] for s in text]\n",
    "        return text\n",
    "    \n",
    "    def decode_token(self, tok):\n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        if isinstance(tok, int):\n",
    "          k = chr(tok)\n",
    "          if k == self.ktos[k]:\n",
    "            return k\n",
    "          else:\n",
    "            return self.decode_token(tuple(ord(ch) for ch in self.ktos[k]))\n",
    "        elif isinstance(tok, tuple):\n",
    "          return \"\".join([self.decode_token(k) for k in tok])\n",
    "        else:\n",
    "          raise TypeError(\" \".join([\"decode_token() accepts only str and tuple\",\n",
    "                          \"instances, not\", str(type(tok))]))\n",
    "            \n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "z3LtAowEye6N",
    "outputId": "24d6b622-02fe-495d-918b-529b4c0463f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.3 s, sys: 1.38 s, total: 28.7 s\n",
      "Wall time: 28.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_text = bpe.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7olBvkPuypv7",
    "outputId": "6fd04095-8d37-46d3-818a-66947cc98179"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26766"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8g4pTj9SCMFx"
   },
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_text[0]) == text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xmg7MQ0eAyVo"
   },
   "source": [
    "## 3. Train 3-gram language model with laplace smoothing δ=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gA2hcvNhsbe6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q70LVSuCCMF5"
   },
   "outputs": [],
   "source": [
    "class LM:\n",
    "    def __init__(self, vocab_size, start_id, end_id, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        self.infer_struct = {}\n",
    "        self.proba = self.infer_struct\n",
    "        self.start_id = start_id\n",
    "        self.start_token = chr(start_id)\n",
    "        self.end_id = end_id\n",
    "        self.end_token = chr(end_id)\n",
    "\n",
    "\n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        result = {ord(ch): self.get_proba(a, b, ch, tau) for ch in self.vocab}\n",
    "        return result\n",
    "\n",
    "    def get_raw_proba(self, a, b, c, tau=1):\n",
    "        smoothed_proba = self.get_proba(a, b, c, tau)\n",
    "        return pow(math.e, smoothed_proba)\n",
    "\n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        if not isinstance(a, str):\n",
    "          if isinstance(a, int):\n",
    "            a = chr(a)\n",
    "          else:\n",
    "            return TypeError(\"Only chars and integers accepted as indices\")\n",
    "        if len(a) != 1:\n",
    "          return ValueError(\"String indices must be chars (have length 1)\")\n",
    "        if a not in self.vocab:\n",
    "          return ValueError(\"Unknown token: \"+a+\" (ord value: \"+str(ord(a))+\")\")\n",
    "        if not isinstance(b, str):\n",
    "          if isinstance(b, int):\n",
    "            b = chr(b)\n",
    "          else:\n",
    "            return TypeError(\"Only chars and integers accepted as indices\")\n",
    "        if len(b) != 1:\n",
    "          return ValueError(\"String indices must be chars (have length 1)\")\n",
    "        if b not in self.vocab:\n",
    "          return ValueError(\"Unknown token: \"+b+\" (ord value: \"+str(ord(b))+\")\")\n",
    "        if not isinstance(c, str):\n",
    "          if isinstance(c, int):\n",
    "            c = chr(c)\n",
    "          else:\n",
    "            return TypeError(\"Only chars and integers accepted as indices\")\n",
    "        if len(c) != 1:\n",
    "          return ValueError(\"String indices must be chars (have length 1)\")\n",
    "        if c not in self.vocab:\n",
    "          return ValueError(\"Unknown token: \"+c+\" (ord value: \"+str(ord(c))+\")\")\n",
    "        \n",
    "        b_prob = pow(self.infer_struct[a][b][\"total\"] + self.delta*len(self.vocab), 1/tau)\n",
    "        t_prob = pow(self.infer_struct[a][b][c] + self.delta, 1/tau)\n",
    "        result = math.log(t_prob/b_prob)\n",
    "        return result\n",
    "    \n",
    "    def fit(self, text):\n",
    "        \"\"\"\n",
    "        train language model on text\n",
    "        text: list of lists\n",
    "        \"\"\"\n",
    "        bigram_count = Counter()\n",
    "        trigram_count = Counter()\n",
    "        self.vocab = set()\n",
    "        for el in text:\n",
    "          s = self.start_token + \"\".join([chr(k) for k in el]) + self.end_token\n",
    "          self.vocab.update(set(s))\n",
    "          bigram_count += Counter([s[i:i+2] for i in range(len(s)-1)])\n",
    "          trigram_count += Counter([s[i:i+3] for i in range(len(s)-2)])\n",
    "        bigram_hits = {}\n",
    "        bigrams = [\"\".join(list(res)) for res in product(self.vocab, self.vocab)]\n",
    "        for bigram in bigrams:\n",
    "          bigram_hits[bigram] = bigram_count[bigram]\n",
    "        trigram_hits = {}\n",
    "        trigrams = [\"\".join(list(res)) for res in product(self.vocab, self.vocab, self.vocab)]\n",
    "        for trigram in trigrams:\n",
    "          trigram_hits[trigram] = trigram_count[trigram]\n",
    "        self.infer_struct = {}\n",
    "        for a in self.vocab:\n",
    "          self.infer_struct[a] = {}\n",
    "          for b in self.vocab:\n",
    "            self.infer_struct[a][b] = {}\n",
    "            self.infer_struct[a][b][\"total\"] = bigram_hits[a+b]\n",
    "            for c in self.vocab:\n",
    "              self.infer_struct[a][b][c] = trigram_hits[a+b+c]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ST-k__i-g22"
   },
   "outputs": [],
   "source": [
    "start_token = max(ord(ch) for ch in bpe.ktos.keys()) + 1\n",
    "end_token = start_token + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "heHkA0up-Y7Z",
    "outputId": "926559fd-43fe-4387-852e-254c1bdf05e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 3s, sys: 76.6 ms, total: 1min 3s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lm = LM(vocab_size, start_token, end_token, 1).fit(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mu26sKAcA5Sj"
   },
   "source": [
    "## 4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jb1HNjqUCMF8"
   },
   "outputs": [],
   "source": [
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_rays(keys_dict, lm, k, tau):\n",
    "      res_dict = {}\n",
    "      for ks in keys_dict:\n",
    "        res = lm.infer(ks[-2], ks[-1], tau)\n",
    "        res_dict.update({ks+chr(ky): res[ky] for ky in res})\n",
    "      return {ks: res_dict[ks] + keys_dict[ks[:-1]]\n",
    "              for ks in sorted(res_dict, key=res_dict.get, reverse=True)[:k]}\n",
    "\n",
    "    if len(input_seq) < 2:\n",
    "      raise ValueError(\"input_seq should be at least 2 elements long, got \" +\n",
    "                       str(input_seq) + \" instead\")\n",
    "\n",
    "    start_id = lm.start_id\n",
    "    end_id = lm.end_id\n",
    "\n",
    "    if input_seq[0] != start_id:\n",
    "      input_seq = [start_id] + input_seq\n",
    "      max_len += 1\n",
    "\n",
    "    input_seq = \"\".join([chr(ch) for ch in input_seq])\n",
    "\n",
    "    if len(input_seq) == 2:\n",
    "      query = lm.infer(input_seq[0], input_seq[1], tau)\n",
    "      proba = query[sorted(query, key=query.get, reverse=True)[0]]\n",
    "    else:\n",
    "      proba = lm.get_proba(input_seq[0], input_seq[1], input_seq[2], tau)\n",
    "      for i in range(3, len(input_seq)):\n",
    "        proba += lm.get_proba(input_seq[i-2], input_seq[i-1], input_seq[i], tau)\n",
    "\n",
    "    best_matches = []\n",
    "    current_beam = {input_seq: proba}\n",
    "    \n",
    "    for i in range(len(input_seq), max_len):\n",
    "        new_beam = get_rays(current_beam, lm, k, tau)\n",
    "        for m in new_beam:\n",
    "          if m.endswith(chr(end_id)):\n",
    "            best_matches.append([m, new_beam[m]])\n",
    "        current_beam = {m: new_beam[m] for m in new_beam\n",
    "                        if not m.endswith(chr(end_id))}\n",
    "        if len(best_matches) == k:\n",
    "          break\n",
    "    \n",
    "    final_matches = sorted(current_beam, key=current_beam.get, reverse=True)\n",
    "    i = 0\n",
    "\n",
    "    while len(best_matches) < k:\n",
    "      best_matches.append([final_matches[i], current_beam[final_matches[i]]])\n",
    "      i += 1\n",
    "\n",
    "    best_matches = [[[ord(ch) for ch in\n",
    "                      m[0].replace(chr(start_id), \"\").replace(chr(end_id), \"\")],\n",
    "                     m[1]] for m in best_matches]\n",
    "\n",
    "    return best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "1QsDl_XizC3K",
    "outputId": "b9f6a73d-86ec-49a5-d632-866dd03a2ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any:\t-74.35749344668162\n",
      "anya:\t-109.41881619508852\n",
      "anyone:\t-87.49836010663222\n",
      "anywho had been:\t-151.25910593581406\n",
      "anywho had beg:\t-155.02020691961656\n",
      "anywho had bef:\t-156.73747682324216\n",
      "anyoney who had :\t-159.71480566880385\n",
      "anybold been :\t-163.39228343621178\n",
      "anyoney who was :\t-166.1324078055385\n",
      "anybothere who :\t-173.5104235712026\n"
     ]
    }
   ],
   "source": [
    "input1 = 'any'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "\n",
    "for sequence, proba in result:\n",
    "  print(bpe.decode(sequence), proba, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "9fY24ANACMGA",
    "outputId": "325c6b7e-4edc-4e9c-ae56-580a7ec50de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horse prince:\t-151.2079940555816\n",
      "horse said no:\t-143.8161743689936\n",
      "horse princes:\t-144.3044552290383\n",
      "horse prince an:\t-144.6350576832909\n",
      "horse who were :\t-150.5379091721297\n",
      "horse princes :\t-150.6387346876215\n",
      "horse who had b:\t-152.68927425123724\n",
      "horse they were :\t-155.69354562382085\n",
      "horse they who :\t-169.4443898136467\n",
      "horse they when:\t-172.87277684578956\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "\n",
    "for sequence, proba in result:\n",
    "  print(bpe.decode(sequence), proba, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "S5Y1Ru9yCMGJ",
    "outputId": "5687a8fe-9e60-4b6c-b8ee-009cacbc4950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her:\t-40.27213839409248\n",
      "here:\t-67.1249462801872\n",
      "hers:\t-59.37099832287781\n",
      "heroom:\t-91.99568974416749\n",
      "herience:\t-141.09601007054923\n",
      "herence andrew :\t-135.36956219788482\n",
      "herkónya did :\t-147.54892237511427\n",
      "herience andrew:\t-149.8819160036072\n",
      "herkónskill :\t-150.78125079375124\n",
      "herence andress:\t-151.30963155414918\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "\n",
    "for sequence, proba in result:\n",
    "  print(bpe.decode(sequence), proba, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "jaJwbRpTCMGN",
    "outputId": "65cf2e08-f43e-4174-d561-8d84d4f37f84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what:\t-7.588823296598268\n",
      "whats:\t-9.683769024814069\n",
      "whatself:\t-12.819002208709813\n",
      "whatimpers:\t-16.381394482242\n",
      "whatraid not :\t-13.397609923140392\n",
      "whatimperself :\t-16.832177198515293\n",
      "whatched the could :\t-17.238741276340953\n",
      "whatself-cam:\t-17.613900401193156\n",
      "whatself-cau:\t-17.951896641157344\n",
      "whatched the cound :\t-18.049154477895215\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "\n",
    "for sequence, proba in result:\n",
    "  print(bpe.decode(sequence), proba, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "2n7BTditCMGT",
    "outputId": "919c023b-2d4a-4195-d33a-3381203d7162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gun proom:\t-189.18812874564293\n",
      "gun prince:\t-179.02182208837888\n",
      "gun princes:\t-172.1182832618356\n",
      "gun prince an:\t-172.44888571608823\n",
      "gun prostóv:\t-177.42428183273356\n",
      "gun princes :\t-178.4525627204188\n",
      "gun who had b:\t-181.27028172654283\n",
      "gun they were :\t-183.915812926423\n",
      "gun proom the :\t-194.4350320724103\n",
      "gun themperor:\t-195.22498170630126\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "\n",
    "for sequence, proba in result:\n",
    "  print(bpe.decode(sequence), proba, sep=\":\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RkSOrMwBBADD"
   },
   "source": [
    "## 5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "co5VTtp7CMGf"
   },
   "outputs": [],
   "source": [
    "def perplexity(snt, lm, raw=False):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    if len(snt) < 2:\n",
    "      raise ValueError(\"snt should be at least 2 elements long, got \" +\n",
    "                       str(snt) + \" instead\")\n",
    "\n",
    "    if len(snt) == 2:\n",
    "      query = lm.infer(snt[0], snt[1])\n",
    "      proba = query[sorted(query, key=query.get, reverse=True)[0]]\n",
    "      if raw:\n",
    "        proba = pow(proba, math.e)\n",
    "    else:\n",
    "      if raw:\n",
    "        proba = lm.get_raw_proba(snt[0], snt[1], snt[2])\n",
    "      else:\n",
    "        proba = lm.get_proba(snt[0], snt[1], snt[2])\n",
    "      for i in range(3, len(snt)):\n",
    "        if raw:\n",
    "          proba += lm.get_raw_proba(snt[i-2], snt[i-1], snt[i])\n",
    "        else:\n",
    "          proba += lm.get_proba(snt[i-2], snt[i-1], snt[i])\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dGemY4od_o2N",
    "outputId": "f753ff77-086e-4668-d97f-de82c9652ada"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-268.46633741055825"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity(tokenized_text[0], lm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
