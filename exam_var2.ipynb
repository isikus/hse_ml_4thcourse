{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exam_var2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgft0pkWp-O",
        "colab_type": "text"
      },
      "source": [
        "## Теоретический вопрос"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN9eBQuaV5Dw",
        "colab_type": "text"
      },
      "source": [
        "**What is the interpretation of Laplace smoothing in n-gram language model?**\n",
        "\n",
        "N-граммная модель рассчитывае вероятность появления n-граммы в тексте как отношение количества вхождений заданной n-граммы в обучающий корпус к общему количеству n-грамм в обучающем корпусе. Проблема, однако, возникает тогда, когда на вход в продакшне поступает n-грамма, которая при обучении не встречалась, либо слово, которое отсутствовало в словаре при обучении. Первая проблема (при одновременном отсутствии второй) обычно решается как подсчёт последовательных вероятностей вхождений (n-1)-грамм с понижающим коэффициента, а для второй применяются различные техники, в числе которых Лапласово сглаживание. Последнее подразумевает добавление всем n-граммвм 1 вхождения: таким образом, количество вхождений поданной на вход n-граммы подразумевается равным реальному + 1 и делится на общее количество n-грамм + количество уникальных n-грамм (n-грамм в вокабуляре)\n",
        "$$P_{Laplace}(w_{i}) = \\frac{c_{i}+1}{N+V}$$\n",
        "Сглаживание Лапласа позволяет избежать деления на ноль, однако, являясь одним из базовых методов решения описанной выше проблемы, на текущий момент устарело и в продакшне используются более вычислительно сложные и эффективные аналоги."
      ]
    }
  ]
}